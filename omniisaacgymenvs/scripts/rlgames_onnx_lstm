

from omniisaacgymenvs.utils.hydra_cfg.hydra_utils import *
from omniisaacgymenvs.utils.hydra_cfg.reformat import omegaconf_to_dict, print_dict
from omniisaacgymenvs.utils.rlgames.rlgames_utils import RLGPUAlgoObserver, RLGPUEnv
from omniisaacgymenvs.utils.task_util import initialize_task
from omniisaacgymenvs.utils.config_utils.path_utils import retrieve_checkpoint_path
from omniisaacgymenvs.envs.vec_env_rlgames import VecEnvRLGames

import hydra
from omegaconf import DictConfig

from rl_games.common import env_configurations, vecenv
from rl_games.torch_runner import Runner

import onnx
import onnxruntime as ort

import datetime
import os
import torch
import numpy as np
from matplotlib import pyplot as plt


class ModelWrapper(torch.nn.Module):
    def __init__(self, model):
        torch.nn.Module.__init__(self)
        self._model = model

    def forward(self, input_dict):
      
        input_dict['obs'] = self._model.norm_obs(input_dict['obs'])
        return self._model.a2c_network(input_dict)


class RLGTrainer:
    def __init__(self, cfg, cfg_dict):
        self.cfg = cfg
        self.cfg_dict = cfg_dict

    def launch_rlg_hydra(self, env):
        self.cfg_dict["task"]["test"] = self.cfg.test

        vecenv.register('RLGPU', lambda config_name, num_actors, **kwargs: RLGPUEnv(config_name, num_actors, **kwargs))
        env_configurations.register('rlgpu', {
            'vecenv_type': 'RLGPU',
            'env_creator': lambda **kwargs: env
        })

        self.rlg_config_dict = omegaconf_to_dict(self.cfg.train)

    def run(self):
        runner = Runner(RLGPUAlgoObserver())
        runner.load(self.rlg_config_dict)
        runner.reset()

        experiment_dir = os.path.join('runs', self.cfg.train.params.config.name)
        os.makedirs(experiment_dir, exist_ok=True)
        with open(os.path.join(experiment_dir, 'config.yaml'), 'w') as f:
            f.write(OmegaConf.to_yaml(self.cfg))

        agent = runner.create_player()
        agent.restore(self.cfg.checkpoint)
        agent.init_rnn()

        import rl_games.algos_torch.flatten as flatten
        inputs = {
            'obs': torch.zeros((1,) + agent.obs_shape).to(agent.device),
            'rnn_states': agent.states,
        }

        with torch.no_grad():
            adapter = flatten.TracingAdapter(ModelWrapper(agent.model), inputs, allow_non_tensor=True)
            traced = torch.jit.trace(adapter, adapter.flattened_inputs, check_trace=False)
            flattened_outputs = traced(*adapter.flattened_inputs)
            # print(flattened_outputs)

        export_file = "onnx_models/Jetbot_dynamic.onnx"

        torch.onnx.export(traced, adapter.flattened_inputs, export_file, verbose=True, 
                          input_names=['obs', 'out_state', 'hidden_state'], output_names=['mu','log_std', 'value', 'out_state', 'hidden_state'])


        onnx_model = onnx.load(export_file)
        onnx.checker.check_model(onnx_model)

        ort_model = ort.InferenceSession(export_file)

        outputs = ort_model.run(
            None,
            {"obs": np.zeros((1,) + agent.obs_shape).astype(np.float32), "out_state.1" : agent.states[0].cpu().numpy(), "hidden_state.1" : agent.states[1].cpu().numpy()},
)
        agent.init_rnn()

        is_done = False
        env = agent.env
        # obs = env.reset()
        obs = env.reset()
        total_reward = 0
        num_steps = 0

        while not is_done:
            outputs = ort_model.run(None, {"obs": obs["obs"].cpu().numpy(),"out_state.1" : agent.states[0].cpu().numpy(), "hidden_state.1" : agent.states[1].cpu().numpy()},)
            print(outputs)
            print("Actions from agent" + str(agent.get_action(obs["obs"], True)))
            mu = outputs[0]
            # print(mu)
            sigma = np.exp(outputs[1])
            action = np.random.normal(mu, sigma)
            action = np.clip(action, -1.0, 1.0)
            action = torch.tensor(action)
            print(obs)
            obs, reward, done, info = env.step(action.to(obs["obs"].device))
            # print(obs, reward, done, info )
            total_reward += reward
            num_steps += 1
            is_done = done

        print(total_reward, num_steps)


@hydra.main(config_name="config", config_path="../cfg")
def parse_hydra_configs(cfg: DictConfig):
    time_str = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    headless = cfg.headless
    env = VecEnvRLGames(headless=headless, sim_device=cfg.device_id)

    if cfg.checkpoint:
        cfg.checkpoint = retrieve_checkpoint_path(cfg.checkpoint)
        if cfg.checkpoint is None:
            quit()

    cfg_dict = omegaconf_to_dict(cfg)
    print_dict(cfg_dict)

    if cfg_dict["test"]:
        cfg_dict["task"]["env"]["numEnvs"] = 1
        cfg_dict["train"]["params"]["config"]["minibatch_size"] = cfg_dict["train"]["params"]["config"]["horizon_length"]

    task = initialize_task(cfg_dict, env)

    from omni.isaac.core.utils.torch.maths import set_seed
    cfg.seed = set_seed(-1, torch_deterministic=cfg.torch_deterministic)

    rlg_trainer = RLGTrainer(cfg, cfg_dict)
    rlg_trainer.launch_rlg_hydra(env)
    rlg_trainer.run()
    env.close()


if __name__ == '__main__':
    parse_hydra_configs()